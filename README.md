# Databricks 14 Days AI Challenge ðŸ§±âš¡ï¸

This repository is a day-by-day record of my progress during the **Databricks 14 Days AI Challenge** (organized by Codebasics and Indian Data Club).

As part of my **#AIReset** 12-week plan, I am using this challenge to bridge the gap between classical ML/Deep Learning and scalable, enterprise-level Data Intelligence pipelines.

## ðŸŽ¯ The Goal
Mastering the Databricks ecosystem to build, deploy, and manage scalable AI solutions.

## ðŸ“‚ Project Structure
I am saving my progress in daily folders containing notebooks (`.ipynb` or `.py`), documentation, and any exported assets.

* [Day 01](./Day01) - Introduction & Environment Setup
* [Day 02](./Day02) - Databricks Architecture & Workspace
* [Day 03](./Day03) - PySpark Transformations Deep Dive
* [Day 04](./Day04) - Delta Lake Introduction
* [Day 05](./Day05) - Delta Lake Advanced
* [Day 06](./Day06) - Medallion Architecture
* [Day 07](./Day07) - Workflows & Job Orchestration
* [Day 08](./Day08) - Unity Catalog Governance
* [Day 09](./Day09) - SQL Analytics & Dashboards
* [Day 10](./Day10) - Performance Optimization
*(Continuing until Day 14)*

## ðŸ›  Tech Stack
* **Cloud Platform:** Databricks
* **Data Handling:** PySpark, Delta Lake, SQL
* **ML & Ops:** MLflow, Unity Catalog
* **Core Skills:** Data Engineering Pipelines & Scalable Machine Learning

## âœï¸ Daily Progress Log
| Day | Focus Area | Key Takeaway |
|:---:|:---|:---|
| 01 | Setup & Workspace | Initialized Databricks clusters and community edition. |
| 02 | Data Loading | Configured Kaggle credentials, loaded e-commerce data from CSV files using shell commands, and explored the dataset using Spark SQL. |
| 03 | PySpark Transformations | Used window functions for running totals, created derived features with joins, and calculated time-based features. |
| 04 | Delta Lake | Converted CSV to Delta format, enforced schema, and handled upserts with MERGE. |
| 05 | Delta Lake Advanced | Performed incremental MERGE, time travel, OPTIMIZE, and VACUUM. |
| 06 | Medallion Architecture | Implemented Bronze, Silver, and Gold layers for data processing. |
| 07 | Workflows & Job Orchestration | Parameterized notebooks with widgets and created a multi-task job to automate the Bronze -> Silver -> Gold pipeline. |
| 08 | Unity Catalog Governance | Used Unity Catalog to create schemas, manage table permissions with `GRANT`/`REVOKE`, and implemented row-level security using a secure view. |
| 09 | SQL Analytics & Dashboards | Created a mock dataset and used advanced SQL (window functions, CTEs) to analyze revenue trends, category performance, and sales funnels. |
| 10 | Performance Optimization | Analyzed query plans, partitioned large tables, applied Z-ordering, and benchmarked performance improvements to understand data skipping. |

---
### ðŸ”— Connect with me
I'm documenting this journey daily on LinkedIn. Let's connect!
ðŸ‘‰ [Rohit Gomes on LinkedIn](https://www.linkedin.com/in/rohit-gomes-12209620a)

#DatabricksWithIDC #Codebasics #AIReset #MLOps| Day | Focus Area | Key Takeaway |
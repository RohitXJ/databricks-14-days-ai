# Databricks 14 Days AI Challenge ğŸ§±âš¡ï¸

This repository is a day-by-day record of my progress during the **Databricks 14 Days AI Challenge** (organized by Codebasics and Indian Data Club).

As part of my **#AIReset** 12-week plan, I am using this challenge to bridge the gap between classical ML/Deep Learning and scalable, enterprise-level Data Intelligence pipelines.

## ğŸ¯ The Goal
Mastering the Databricks ecosystem to build, deploy, and manage scalable AI solutions.

## ğŸ“‚ Project Structure
I am saving my progress in daily folders containing notebooks (`.ipynb` or `.py`), documentation, and any exported assets.

* [Day 01](./Day01) - Introduction & Environment Setup
* [Day 02](./Day02) - Databricks Architecture & Workspace
* [Day 03](./Day03) - PySpark Transformations Deep Dive
* [Day 04](./Day04) - ...
*(Continuing until Day 14)*

## ğŸ›  Tech Stack
* **Cloud Platform:** Databricks
* **Data Handling:** PySpark, Delta Lake, SQL
* **ML & Ops:** MLflow, Unity Catalog
* **Core Skills:** Data Engineering Pipelines & Scalable Machine Learning

## âœï¸ Daily Progress Log
| Day | Focus Area | Key Takeaway |
|:---:|:---|:---|
| 01 | Setup & Workspace | Initialized Databricks clusters and community edition. |
| 02 | Data Loading | Configured Kaggle credentials, loaded e-commerce data from CSV files using shell commands, and explored the dataset using Spark SQL. |
| 03 | PySpark Transformations | Used window functions for running totals, created derived features with joins, and calculated time-based features. |

---
### ğŸ”— Connect with me
I'm documenting this journey daily on LinkedIn. Let's connect!
ğŸ‘‰ [Rohit Gomes on LinkedIn](https://www.linkedin.com/in/rohit-gomes-12209620a)

#DatabricksWithIDC #Codebasics #AIReset #MLOps| Day | Focus Area | Key Takeaway |
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f744d85-73a1-4003-a634-0cf6e9031f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DAY 11 : Statistical Analysis & ML Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8add96ca-be4c-491b-a279-56deede52f15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load data and convert event_time string to timestamp for analysis\n",
    "events = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumn(\"event_time\", F.to_timestamp(\"event_time\", \"yyyy-MM-dd HH:mm:ss 'UTC'\")) \\\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "\n",
    "#events.cache() # Keep in memory for faster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29713332-7e57-4bad-b647-3f55b2ad15ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|summary|             price|\n+-------+------------------+\n|  count|          67501979|\n|   mean|292.45931656479536|\n| stddev|355.67449958606727|\n|    min|               0.0|\n|    max|           2574.07|\n+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistical summaries for the price column\n",
    "events.describe([\"price\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de7ac58-4420-475a-9589-3d468184ec98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n|is_weekend|event_type|   count|\n+----------+----------+--------+\n|     false|  purchase|  500258|\n|     false|      view|40453993|\n|      true|      view|23102117|\n|     false|      cart| 1799242|\n|      true|      cart| 1229688|\n|      true|  purchase|  416681|\n+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Test if user behavior changes on weekends (1=Sunday, 7=Saturday)\n",
    "weekday = events.withColumn(\"is_weekend\", F.dayofweek(\"event_date\").isin([1, 7]))\n",
    "weekday.groupBy(\"is_weekend\", \"event_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ab612b-a62f-4f49-abbe-0f8ef2b9960b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Price and Purchase: 0.0025286683578114658\n"
     ]
    }
   ],
   "source": [
    "# Identify if higher prices correlate with fewer purchases\n",
    "corr_df = events.withColumn(\"is_purchase\", F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0))\n",
    "correlation = corr_df.stat.corr(\"price\", \"is_purchase\")\n",
    "print(f\"Correlation between Price and Purchase: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343dcb32-d465-4a1c-b124-3437e80f0160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------+-----------------+---------------------+\n| user_id|hour|day_of_week|        price_log|time_since_first_view|\n+--------+----+-----------+-----------------+---------------------+\n|65800726|   4|          4|4.416428061391214|                    0|\n|65800726|   4|          4|4.416428061391214|                  128|\n|81255481|   7|          6|4.209902902856373|                    0|\n|81255481|  14|          5|4.206779991551889|              1146401|\n|82079354|   4|          5|5.155831718251282|                    0|\n+--------+----+-----------+-----------------+---------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Engineer temporal and behavioral features for model training\n",
    "# Use unix_timestamp on both sides of the subtraction to avoid BIGINT vs TIMESTAMP mismatch\n",
    "features = events.withColumn(\"hour\", F.hour(\"event_time\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"event_date\")) \\\n",
    "    .withColumn(\"price_log\", F.log(F.col(\"price\") + 1)) \\\n",
    "    .withColumn(\"time_since_first_view\", \n",
    "        F.unix_timestamp(\"event_time\") - \n",
    "        F.unix_timestamp(F.first(\"event_time\").over(Window.partitionBy(\"user_id\").orderBy(\"event_time\"))))\n",
    "\n",
    "features.select(\"user_id\", \"hour\", \"day_of_week\", \"price_log\", \"time_since_first_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aab34ee-e2e0-41b6-af29-7423ab1d7209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data saved to workspace.ml_prep_lab.engineered_features\n"
     ]
    }
   ],
   "source": [
    "# Create the schema if it was missed in this session and save the table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.ml_prep_lab\")\n",
    "\n",
    "features.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"workspace.ml_prep_lab.engineered_features\")\n",
    "\n",
    "print(\"Success: Data saved to workspace.ml_prep_lab.engineered_features\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}